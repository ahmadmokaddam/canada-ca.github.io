---
layout: post
title:  "Finding the right method to measure online task success"
pubdate: "February 5, 2020"
langpage: "https://blogue.canada.ca/2020/02/05/methode-pour-mesurer.html"
date:   2020-02-05
published: true
draft: true
lang: en
alt: "right method to measure"
description: "What we learned about exit surveys, usability testing and analytics funnels"
---

In 2019, the Digital Transformation Office worked with our colleagues in the Results Division of Treasury Board Secretariat to put in place a new key performance indicator. It evaluates citizens’ ability to complete high-demand tasks on the Government of Canada’s web presence. 37 departments had to report on the new indicator in 2019.

Measuring online task completion helps us understand how design improvements affect task success. It also helps institutions adopt a top task approach to managing online information and services.

As we analyzed the results early in 2020, we learned a lot about the different methods for measuring online task success.

## Background

Improving online top tasks aligns with the [Government of Canada’s Digital Standard](https://www.canada.ca/en/government/system/digital-government/government-canada-digital-standards.html) on empowering staff to deliver better services.

<blockquote><p style="color: #1E5D71 !important;">“Change the key metrics and you will change the organization. Change the organization and you will deliver vastly better digital services.”</p>

<footer><a href="http://blog.canada.ca/2018/02/23/improve-digital-measuring-outcomes.html">Gerry McGovern, February 2018</a></footer></blockquote>

In the spirit of experimentation and respect for available resources, the Digital Transformation Office offered 3 methods to complete this report:

* usability testing
* task funnel analysis
* task completion survey

## How tasks were measured across the reporting institutions

We compiled the data that each institution sent. We tagged each task with the collection method, number of participants, time period, and success rate.  Here’s what we found.

<b>Reporting method</b>
* 9 surveys
* 8 usability tests
* 7 task funnels
* 2 other methods

<b>Types of tasks reported </b>
* 283 reported tasks
* Over 200 tasks about looking for answers to questions
* 23 tasks about applying, renewing, filing, or registering
* 15 tasks about how to get in contact
* 15 tasks about seeking jobs and careers
* 18 tasks about downloading, signing in, purchasing, or calculating

#### Task success

We looked at the success rate for all tasks that were tested with over 20 participants. 101 tasks met this criteria.
* The average success rate for these 101 tasks was 64%
* 32 tasks had a high success rating of over 80%

9 institutions had high success tasks. These were mostly answer-finding tasks. Topics included border wait times, career opportunities, and travel health notices.

Lower performing tasks often included applying or registering for things. It’s difficult to pinpoint the exact cause of the lower success rates. Possible causes include:

* the type of reporting method (task funnels were often used)
* the nature of the task being more complex with both online and offline requirements
* issues with the actual online experience

## The pros and cons of each reporting method

Here’s what we learned about each reporting method.

### Usability testing

Moderated usability testing is a fantastic way to learn about how people use your site, and to see where there are problems. It’s also the gold standard for measuring task success. But, usability tests focused on task performance are a bit different from standard usability testing. The main difference is that task performance testing tends to use more participants (at least 16). It focuses both on arriving at a reliable success score per task and on learning about how people use the site.

Moderated usability testing results are difficult to compare across many institutions. They rely on phrasing tasks and scenarios consistently. As well, the cost, time and expertise needed to run and analyze rigorous usability tests can be challenging for smaller institutions.

That said, nothing can replace watching real people use your site. We always recommend incorporating usability testing into your web team’s toolkit.

### Task funnel

This option gave institutions the ability to report on a single task. They used anonymized web analytics to track visitors’ paths from beginning a task, to the successful completion.

Funnels work best for tracking conversions over multiple pages where there is a single sequence of steps to follow (such as an e-commerce checkout). The drawback is that people drop out of funnels for reasons that have nothing to do with the effectiveness of the experience.

This method is not great for answer-finding tasks. These often have multiple path options making them difficult or impossible to track.

Several of those who used this method also noted that they found it challenging because of outdated analytics platforms and the inability to track users through secure areas of a website.  

We found that the quality of data we received from task funnels was mixed. This method was sometimes confused with a findability study. Users were asked if they were able to find links from a defined page. There are better methods to determine findability such as [tree testing or click testing](https://www.nngroup.com/articles/navigation-ia-tests/).

### Online survey

An online survey was the preferred method for collecting top task success data. We found it delivered comparable results across institutions. For institutions beginning their journey in top task management, the survey method is the easiest to implement and rerun year after year.

The Digital Transformation Office developed a survey that any institution could use. Institutions only needed to write their task scenarios and allow sufficient time to run the survey. This survey was the only method that allowed write-in responses. That gave institutions the chance to uncover new or under-represented tasks.  

Results from the surveys were automatically collected into dashboards. This further reduced the institution’s workload for compiling and analyzing the data.

The Digital Transformation Office survey prompted visitors to:

* select the task that they came to do (including a write-in option in case their task wasn’t listed)
* confirm if they were able to complete their task
* select why they were not able to complete their task (including a write-in option) if they couldn’t

We found that this method allowed the best balance between:

* standardizing data
* helping institutions understand why people visit their pages
* pinpointing the parts of the online experience causing service delivery issues

The first round of reports uncovered challenges in terms of ease of execution and data quality.  We’re taking these findings into consideration as we plan the 2020 exercise.

## Simplifying our methods for better results

Offering multiple methods to report on top tasks was meant to remove barriers to data collection. The reality was that it affected our ability to see trends across institutions. The different methods created inconsistent data. It’s hard to compare the results of a task funnel to a usability test!  

## Moving forward: a single reporting method for 2020

It’s clear that a single top task reporting method would help us see the big picture of online task delivery across the Government of Canada.

The online survey method provides a standardized data collection method. It is technically easy to implement, and provides clear data about top task strengths and weaknesses.

The feedback we heard from institutions using this method was positive. It indicated a culture shift to top task management.

<blockquote><p style="color: #1E5D71 !important;">“The survey allows us to prioritize which section on the website should be optimized first, for example our focus/energy should go towards top tasks that are not performing as [well] as the other ones.”</p> </blockquote>

<blockquote><p style="color: #1E5D71 !important;"> “Over the course of time, the findings of WES (Web Experience Survey) have played a part in the evolution of the website.”</p> </blockquote>

For the next task reporting request in 2020, we will ask that institutions use an online survey.

## We want to hear from you

What would make online top task reporting easier for you to complete?

* tips and best practices for writing top task survey questions
* advice on how to break down big tasks into smaller, measurable steps
* instructions on what to do if your service includes online and offline delivery channels
* other suggestions?

## Inspired by what you learned? Share this post with your team.

 Connect with the Digital Transformation Office at TBS:
* Email: [dto.btn@tbs-sct.gc.ca](mailto:dto.btn@tbs-sct.gc.ca)
* Twitter: #Canadadotca (English) / #Canadapointca (French)
* Slack: [http://design-GC-conception.slack.com](https://design-gc-conception.slack.com/join/shared_invite/enQtODE1OTc5Mzg5NzQ4LWQ3MjZjMTdjMjk2ZTZmMTJjYWQ3ZmRiNDYwYjRmN2NjYzQyNjFlNDBlY2FkNWE1ODg2YjExY2QwZmVjN2MwMGM)

## Learn more

* [Improve digital services by measuring outcomes](https://blog.canada.ca/2018/02/23/Improve-digital-services-measuring-outcomes.html)
* [How we identify top tasks for Canada.ca](https://blog.canada.ca/2017/12/11/top-100-for-gc.html)
* [How measuring task success is key for optimizing Canada.ca top tasks](https://blog.canada.ca/2017/12/12/optimization-overview.html)
