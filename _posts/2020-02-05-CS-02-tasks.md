---
layout: post
title:  "Finding the right method to measure online task success"
pubdate: "February 5, 2020"
langpage: "https://blog.canada.ca/2020/02/05/CS-02-tasks.html"
date:   2020-02-05
published: true
draft: true
lang: en
alt: "CS-02 tasks"
description: "What we learned from the CS-02 online task success reports"
---

In 2019, the Digital Transformation Office worked with our colleagues in the Results Division of Treasury Board Secretariat to put in place a new key performance indicator. It evaluates citizens’ ability to complete high-demand tasks on the Government of Canada’s web presence. 37 departments had to report on the new indicator in 2019.

Measuring online task completion helps us understand how design improvements affect task success. It also helps institutions adopt a top task approach to managing online information and services.

As we analyzed the results early in 2020, we learned a lot about the different methods for measuring online task success.

## Background

The official name of the new indicator is “Communications Services - Outcome 2 (CS-O2).” It is part of the internal services indicators under the [Policy on Results](https://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=31300).  

Reporting on this indicator pushes Government of Canada institutions to make service delivery a priority. It helps them plan their digital work around improving success for top tasks online. This aligns with the [Government of Canada’s Digital Standard](https://www.canada.ca/en/government/system/digital-government/government-canada-digital-standards.html) on empowering staff to deliver better services.

<blockquote><p style="color: #1E5D71 !important;">“Change the key metrics and you will change the organization. Change the organization and you will deliver vastly better digital services.”</p>
</blockquote>

<p>- [Gerry McGovern, February 2018](https://blog.canada.ca/2018/02/23/Improve-digital-services-measuring-outcomes.html) </p>

In the spirit of experimentation and respect for available resources, the Digital Transformation Office offered 3 methods to complete this report:

* usability testing
* task funnel analysis
* task completion survey

## How tasks were measured across the reporting institutions

We compiled the data that each institution sent. We tagged each task with the collection method, number of participants, time period, and success rate.  Here’s what we found.

<b>Reporting method</b>
* 9 Surveys
* 8 Usability tests
* 7 Task funnels
* 2 Other methods

<b>Types of tasks reported </b>
* 283 reported tasks
* Over 200 tasks about looking for answers to questions
* 23 tasks about applying, renewing, filing, or registering
* 15 contact us tasks
* 15 jobs and careers seeking tasks
* 18 tasks about downloading, signing in, purchasing, or calculating

#### Task success
We looked at the success rate for all tasks that were tested with over 20 participants. 101 tasks met this criteria.
* The average success rate for these 101 tasks was 64%
* 32 tasks had a high success rating of over 80%

9 institutions had high success tasks. These were mostly answer-finding tasks. Topics included border wait times, career opportunities, and travel health notices.

Lower performing tasks often included applying or registering for things. It’s difficult to pinpoint the cause of the lower success rates. Possible causes include:

* the type of reporting method (task funnels were often used)
* the nature of the task being more complex with both online and offline requirements
* issues with the actual online experience

## The pros and cons of each reporting method

Here’s what we learned about each reporting method.

### Usability testing

Moderated usability testing is a fantastic way to learn about how people use your site, and to see where there are problems. It’s also the gold standard for measuring task success. But, usability tests focused on task performance are a bit different from standard usability testing. The main difference is that task performance testing tends to use more participants (at least 16). It focuses both on arriving at a reliable success score per task and on learning about how people use the site.

Moderated usability testing results are difficult to compare across many institutions. They rely on phrasing tasks and scenarios consistently. As well, the cost, time and expertise needed to run and analyze rigorous usability tests can be challenging for smaller institutions.

That said, nothing can replace watching real people use your site. We always recommend incorporating usability testing into your web team’s toolkit.

### Task funnel

This option gave institutions the ability to report on a single task. They used anonymized web analytics to track visitors’ paths from beginning a task, to the successful completion.

Funnels work best for tracking conversions over multiple pages where there is a single sequence of steps to follow (such as an e-commerce checkout). The drawback is that people drop out of funnels for reasons that have nothing to do with the effectiveness of the experience.

This method is not great for answer-finding tasks. These often have multiple path options making them difficult or impossible to track.

Several of those who used this method also noted that they found it challenging because of outdated analytics platforms and the inability to track users through secure areas of a website.  

We found that the quality of data we received from task funnels was mixed. This method was sometimes confused with a findability study. Users were asked if they were able to find links from a defined page. There are better methods to determine findability such as [tree testing or click testing](https://www.nngroup.com/articles/navigation-ia-tests/).










## Inspired by what you learned? Share this post with your team.

 Connect with the Digital Transformation Office at TBS:
* Email: [dto-btn@tbs-sct.gc.ca](mailto:dto-btn@tbs-sct.gc.ca)
* Twitter: #Canadadotca
